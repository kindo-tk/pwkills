{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007cd7f7-760e-4015-8488-73b1be3186e5",
   "metadata": {},
   "source": [
    "## Q1. Define Artificial intelligence AI ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc334b98-e0dc-456e-9f9e-f1314f83eeb0",
   "metadata": {},
   "source": [
    "It is a branch of computer science that focuses on building intelligent systems that perform tasks, that require intelligence when performed by humans.\n",
    "    \n",
    "Ex :- Robot,self driving car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e788eed-3962-4895-9b8a-cfbc6d3e84bb",
   "metadata": {},
   "source": [
    "## Q2. Explain the differences between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b9c09-78dc-45a1-892f-7434e29bd6d2",
   "metadata": {},
   "source": [
    "     Artificial Intelligence(AI):\n",
    "smart application that can perform own task without human intervention.\n",
    "        \n",
    "EX:- Robot , self Drvining car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436404b3-ed0f-425d-bad4-b463969dd081",
   "metadata": {},
   "source": [
    "    Machine Learning :\n",
    "\n",
    "Machine learn pattern from data and tried to replicate same in future.\n",
    "        \n",
    "Ex.Spam & Ham ,Diabetes or not ,price of house"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b83a8-cddf-4fd0-9019-8f57c02cb2a7",
   "metadata": {},
   "source": [
    "    Deep Learning :\n",
    "\n",
    "Specialized machine learning algorithms that mimic human brain.\n",
    "    \n",
    "Ex: chat gpt ,devin AI,Object detections,Image recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0cd9e-73ab-42c9-b81d-46b8f2213ee5",
   "metadata": {},
   "source": [
    "        Data Science :\n",
    "\n",
    "Interdisciplinary field comprised of statistics,math,and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678abf44-3f13-4a6c-a696-fea115bc0b32",
   "metadata": {},
   "source": [
    "## Q3. How does AI differ from traditional software developments ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f7f60-3408-49e8-92b8-7f7ac786f64c",
   "metadata": {},
   "source": [
    "AI differs from traditional software development in several fundamental ways. Here are the key differences:\n",
    "\n",
    "### 1. **Programming Paradigm**:\n",
    "   - **Traditional Software**: Follows a rule-based paradigm where developers explicitly program all the rules and logic needed to perform specific tasks. The software performs actions based on these predefined instructions.\n",
    "   - **AI Development**: Focuses on data-driven approaches. Instead of explicitly coding all rules, AI systems learn patterns and make decisions based on data. Machine learning (ML) models are trained on large datasets to find patterns and make predictions or decisions.\n",
    "\n",
    "### 2. **Learning and Adaptation**:\n",
    "   - **Traditional Software**: Once programmed, the software behavior remains static unless it is manually updated by developers. The system does not learn or adapt from new data.\n",
    "   - **AI Systems**: Continuously learn and improve from new data. AI models can adapt to new information without human intervention, allowing them to handle changing environments and tasks better.\n",
    "\n",
    "### 3. **Handling Complexity**:\n",
    "   - **Traditional Software**: Handles straightforward and well-defined problems where all possible inputs and outputs are known. It relies on deterministic logic.\n",
    "   - **AI Systems**: Are designed to handle complex, uncertain, or poorly defined problems where all possible scenarios cannot be hard-coded. They use probabilistic reasoning and pattern recognition to make decisions in complex environments.\n",
    "\n",
    "### 4. **Development Process**:\n",
    "   - **Traditional Software**: Development is typically linear, involving requirements gathering, design, coding, testing, and maintenance. Each step is well-defined.\n",
    "   - **AI Development**: Involves an iterative process of data collection, data preprocessing, model selection, training, validation, and testing. The development is often non-linear, with frequent experimentation and tuning.\n",
    "\n",
    "### 5. **Output and Debugging**:\n",
    "   - **Traditional Software**: Produces predictable outputs for a given input. Debugging involves tracing the code logic to find errors.\n",
    "   - **AI Systems**: Outputs can be probabilistic and not always explainable. Debugging involves understanding why a model made a certain prediction, which can be challenging, especially for complex models like deep neural networks.\n",
    "\n",
    "### 6. **Role of Data**:\n",
    "   - **Traditional Software**: Relies primarily on the algorithms coded by developers. Data is important but usually static and secondary to the code.\n",
    "   - **AI Systems**: Data is a core component. The quality and quantity of data directly impact the model's performance. AI models need vast amounts of labeled data for training and testing.\n",
    "\n",
    "### 7. **Decision Making**:\n",
    "   - **Traditional Software**: Decisions are made based on clearly defined rules and conditions.\n",
    "   - **AI Systems**: Decisions are made based on learned patterns, probabilities, and statistical models. The decision-making process can often be opaque or a \"black box.\"\n",
    "\n",
    "### 8. **Maintenance and Updates**:\n",
    "   - **Traditional Software**: Requires regular updates to add new features or fix bugs, which are explicitly coded by developers.\n",
    "   - **AI Systems**: Require retraining with new data to adapt to changing patterns. Maintenance involves updating models, managing data drift, and ensuring the model remains relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ea8d7-619f-40d2-a194-a06dc10add3e",
   "metadata": {},
   "source": [
    "## Q4. Provide examples of AI, ML, DL, and DS applications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda0493-0553-4dc7-bda6-39d2ea47ab94",
   "metadata": {},
   "source": [
    "AI is used in virtual assistants, recommendation systems, and more.\n",
    "    \n",
    "ML is applied in image recognition, spam filtering, and other data tasks.\n",
    "    \n",
    "DL is utilized in autonomous vehicles, speech recognition, and advanced AI applications.\n",
    "    \n",
    "DS is utilized In Search Engines,Transport ,Finance, Ecormerce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f7b67-5a88-4cd6-b256-58cd7d785fa5",
   "metadata": {},
   "source": [
    "## Q5. Discuss the importance of AI, ML, DL, and DS in today's World ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2cb20-fe5b-41a4-84b2-61325f5ab725",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) sets the stage for machines that can simulate human intelligence. Machine Learning (ML) evolves from AI, giving machines the ability to learn and grow from experience. Deep Learning (DL), nestled within ML, drives machines to understand and operate on a level akin to human intuition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1747e-e7de-410e-ac76-dc562f66452c",
   "metadata": {},
   "source": [
    "## Q6. what is supervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35541b1-9ca1-4eee-aaa2-def33b18d311",
   "metadata": {},
   "source": [
    "Supervised learning is a type of machine learning where the model is trained on labeled data. It involves learning a function that maps input variables (independent variables) to an output variable (target variable). The two main problem types in supervised learning are:\n",
    "\n",
    "1. **Regression**: Predicting continuous output values (e.g., predicting house prices).\n",
    "2. **Classification**: Predicting discrete categories or labels (e.g., classifying emails as spam or not spam).\n",
    "\n",
    "The goal is to learn the relationship between input and output variables to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04423f9-bdbb-4bbe-9cd8-b51f40b2e9b6",
   "metadata": {},
   "source": [
    "## Q7 .Provide the examples of supervised learning algorithms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200752e-1a4f-4fb6-93c3-b7d23096602f",
   "metadata": {},
   "source": [
    "The most commonly used Supervised Learning algorithms are decision tree, logistic regression, linear regression, support vector machine.\n",
    "    \n",
    "Example:  supervised learning problems is predicting house prices,diabetic or not, spam or ham, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cd7d4-d6c6-4a7f-8fde-890541124a29",
   "metadata": {},
   "source": [
    "## Q8. Explain the process of supervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37cef7-c1d1-4398-9468-1a42ff2e499d",
   "metadata": {},
   "source": [
    "Supervised machine learning algorithm that used labeled dataset which has dependant variable.\n",
    "It is used to explain the relationship between input and output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfbae8-a35c-48a3-a3ec-786908469610",
   "metadata": {},
   "source": [
    "## Q9. what is the characteristics of unsupervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443313f-392b-4bff-9ced-1c6d34a012b7",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning where the model is trained on data without labeled outputs. Here are the key characteristics:\n",
    "\n",
    "1. **No Labeled Data**: Unlike supervised learning, unsupervised learning works with data that has no target or output labels. The model tries to find patterns, structures, or relationships within the input data.\n",
    "\n",
    "2. **Data Exploration**: It is primarily used for exploring the underlying structure of the data, finding hidden patterns, and gaining insights from unstructured data.\n",
    "\n",
    "3. **Clustering and Association**: The main types of problems are:\n",
    "   - **Clustering**: Grouping similar data points together (e.g., customer segmentation).\n",
    "   - **Association**: Finding relationships between variables in large datasets (e.g., market basket analysis).\n",
    "\n",
    "4. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are used to reduce the number of variables while preserving important information.\n",
    "\n",
    "5. **No Direct Feedback**: There is no feedback from labeled data to guide the learning process. The model relies on the inherent structure of the data to make decisions.\n",
    "\n",
    "6. **Applications**: Used in tasks such as anomaly detection, pattern recognition, feature extraction, and data compression.\n",
    "\n",
    "7. **Data-Driven Learning**: The algorithms learn from data patterns without predefined classes or categories, making them suitable for exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb14de-d243-44c4-b0f7-6bf8c1c33383",
   "metadata": {},
   "source": [
    "## Q10. Give examples of unsupervised Learning algorithms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fba5b3-8947-4aaa-8475-322902862ae0",
   "metadata": {},
   "source": [
    "1. clustering \n",
    "2. Fraud detection\n",
    "3. dimensionality reduction\n",
    "4. anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76725cfa-8bb4-4851-98e4-a30d4297395d",
   "metadata": {},
   "source": [
    "## Q11. Describe semi-supervised Learning and its significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c85237-1576-431e-bfe8-d781f4abfc5c",
   "metadata": {},
   "source": [
    "Semi-supervised learning combines a small amount of labeled data with a large amount of unlabeled data for training. It improves model accuracy by leveraging patterns in the unlabeled data while using labeled data to guide learning. This approach reduces the cost and effort of data labeling, making it ideal for real-world applications like web content classification, medical diagnosis, and image recognition, where labeled data is scarce but unlabeled data is abundant. It provides better performance than unsupervised learning alone and is data-efficient in low-resource settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c5554-212a-4bfc-b95a-b415b3dc9c5e",
   "metadata": {},
   "source": [
    "## Q12. Explain Reinforcement Learning and its applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd20e4-0bee-4dee-9e17-e3582b5803d6",
   "metadata": {},
   "source": [
    "Reinforcement learning is an area of ml concerned with how intelligent agent ought to take action in  an environment\n",
    "Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07377b2d-15b1-43f4-b17a-e8c8bc85c7b8",
   "metadata": {},
   "source": [
    "## Q13. How does Reinforcement Learning differ from supervised and unsupervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03188e8f-bbd2-4150-b2b7-d9645c6fd803",
   "metadata": {},
   "source": [
    "    Supervised learning :        \n",
    "supervised leaning used labeled data to learn pattern or outpu of data.\n",
    "        \n",
    "        \n",
    "    Unsupervised Learning :\n",
    "unsupervised learning uses unlabeled data to learn pattern from data.\n",
    "        \n",
    "        \n",
    "    Reinforcement learning :\n",
    "Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317361b-b9dc-4584-b593-7974d6e44912",
   "metadata": {},
   "source": [
    "## Q14 what is the purpose of the Train-Test-Validation split in Machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dccbe5-28e2-4fe9-9890-ef87913b2ee0",
   "metadata": {},
   "source": [
    "Purpose of train test & validation \n",
    "    \n",
    "    Train :\n",
    "        \n",
    "train data used for training of model.\n",
    "        \n",
    "    Test :\n",
    "        \n",
    "Test data is used for testing the model accuracy\n",
    "        \n",
    "    Validation :\n",
    "        \n",
    "Validation data is used for hyperparameter tuning.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb229b4-cfb7-4a51-a036-d351bc998c00",
   "metadata": {},
   "source": [
    "## Q15. Explain the significance of the training set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdac02-239d-483a-b3eb-52af3dc130ce",
   "metadata": {},
   "source": [
    "Training set is subset of data which is use to train machine learning model learn pattern from data.training data is used to learn the pattern relationship from the data.training set is largest subset of data is used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a46f47-ae68-4f2f-a747-2b482bce0c82",
   "metadata": {},
   "source": [
    "## Q16.How do you determine the size of the training, testing, and validation sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f37eac-6dda-4e63-889a-24ebb5fcc822",
   "metadata": {},
   "source": [
    "    Train :\n",
    "train data size is 60 %. we can train the model using 60% data.\n",
    "        \n",
    "    Test :\n",
    "Test data size is 20% .We can test the model accuracy using thise spoecific data.\n",
    "        \n",
    "    Validation data:\n",
    "Validation data size is 20%. we have to use the validation data for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83159490-423a-4c91-a3b0-5899bda8352e",
   "metadata": {},
   "source": [
    "## Q17 what are the consequences of improper Train-Test-Validation splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20bae90-2d3d-4741-b5d9-5c4f5a92fb43",
   "metadata": {},
   "source": [
    "Incorrectly shuffling or sorting the data before splitting can introduce bias and affect the generalization of the final model. For example, if the dataset is not shuffled randomly before splitting into training set and validation set, it may introduce biases or patterns that the model can exploit during training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d79d57-f6c3-4b61-b753-8b52c21e947c",
   "metadata": {},
   "source": [
    "## Q18 Discuss the trade-offs in selecting appropriate split Ratios ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3da49-60b0-4ac7-9d2f-d0a7417c2f12",
   "metadata": {},
   "source": [
    "In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180d7c9-65ee-4343-84d8-eeaae4e9d7c3",
   "metadata": {},
   "source": [
    "## Q19 Define model performance in machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc9801-9a10-43fe-b48a-4a09520fc1de",
   "metadata": {},
   "source": [
    "Model performance in machine learning is how the model giving accurate prediction on unseen data that term is called as model performance.ou typically measure model performance using a test set, where you compare the predictions on the test set to the actual outcomes.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2548f39-01b5-45ce-aa2e-83c1382b8fd9",
   "metadata": {},
   "source": [
    "## Q20. How do you measure the performance of a machine learning model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158151ee-00cf-40e7-80ff-f8ca9df8ecee",
   "metadata": {},
   "source": [
    "The machine learning model can be evaluated using metrics that analyze its ability to make predictions, its weaknesses, and how well it can generalize future predictions\n",
    "    \n",
    "For Regression :\n",
    "we have use: \n",
    "1. mean square error\n",
    "2. mean absolute error\n",
    "3. Root mean square error\n",
    "4. r2 score\n",
    "        \n",
    "for classification :\n",
    "\n",
    "1.confusion matrix\n",
    "\n",
    "2.accuracy score\n",
    "\n",
    "3.classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d525f-5051-4467-9f64-1503fbee1275",
   "metadata": {},
   "source": [
    "## Q21 what is over fitting and why is it problematic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01b0aa-9278-45bf-b7b2-fe4b6f18b778",
   "metadata": {},
   "source": [
    "Model performing well during training data but not during testing that is called as over fitting.\n",
    "   \n",
    "Train ==> while training accuracy is high\n",
    "   \n",
    "test ===> while testing accuracy is low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c77d0f-a82e-419c-874b-644026eafd83",
   "metadata": {},
   "source": [
    "Problem due to over fitting:\n",
    "    \n",
    "In ml while training accuracy is high & testing accuracy is low that is over fitting. over fitting can reduced the model reliability, it can produce inaccurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524013a1-7d6d-4961-abd7-7ea8994a3bf7",
   "metadata": {},
   "source": [
    "## Q22 Provide techniques to address over fitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fa9e5-e314-4bde-840a-f8271bdcd8aa",
   "metadata": {},
   "source": [
    "1.Cross validation\n",
    "\n",
    "2.Evaluation\n",
    "\n",
    "3.Training early stopping\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec7450-3639-45d2-907f-5f2fe06ed2ab",
   "metadata": {},
   "source": [
    "## Q23 Explain under fitting and its implications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311aa0d-9d05-4a2d-ae9e-6258fe215127",
   "metadata": {},
   "source": [
    "Basically Low Train accuracy & low test accuracy is under fitting. Under fitting happen because of \n",
    "where data model is unable to capture the relationship of input and output variable. it causes high error while training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d23836-5810-466d-afae-0b58c8c0e323",
   "metadata": {},
   "source": [
    "## Q24 How can you prevent under fitting in machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b25c8-44ef-4808-8dd8-0fbc357b5ba9",
   "metadata": {},
   "source": [
    "1. Increase model complexity.\n",
    "2. Increase the number of features, performing feature engineering.\n",
    "3. Remove noise from the data.\n",
    "4. Increase the number of epochs or increase the duration of training to get better\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9b9af-9196-4da9-966f-22fa876c5a29",
   "metadata": {},
   "source": [
    "## Q25 Discuss the balance between bias and variance in model performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248fbc9-b801-4f2c-acc2-17f0ae633aac",
   "metadata": {},
   "source": [
    "#### High Bias & Low Bias :-\n",
    "    1. Low training error is also known as low bias\n",
    "    2. high training error is known as high bias\n",
    "    \n",
    "    \n",
    "#### High Variance & Low Variance:\n",
    "    1. low testing error is known as low variance\n",
    "    2. high testing error is known as high variance\n",
    "    \n",
    "#### Over fitting :\n",
    "    1. training accuracy is high ==> low bias\n",
    "    2. testing accuracy is low ==> high variance\n",
    "    \n",
    "#### Under fitting :\n",
    "    1. training accuracy is low ==> high bias\n",
    "    2. testing accuracy is low ==> high variance\n",
    "    \n",
    "#### Generalize model :\n",
    "    1. training accuracy is high ==> low bias\n",
    "    2. testing accuracy is high ==> low variance    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205a132-a6f5-41a7-a569-0d6834afb62e",
   "metadata": {},
   "source": [
    "## Q26 what are the common techniques to handle missing data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c33c94-93fb-4c2a-9be0-b3476b493d4c",
   "metadata": {},
   "source": [
    "1. Missing Completely at random \n",
    "2. Missing At Random\n",
    "3. Missing not at Random\n",
    "   \n",
    "### We can handle missing data using Imputation.\n",
    "1. Numerical Data :- we will use mean & median Imputation\n",
    "2. Categorical Data :- we will use mode imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bcbb5-35dc-411d-b1e3-0123738fb459",
   "metadata": {},
   "source": [
    "## Q27. Explain the implications of ignoring missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce6b26-24d1-40ae-abdf-2f59cbb62eab",
   "metadata": {},
   "source": [
    "Implication of Missing Data:==>\n",
    "    \n",
    "1. sampling Bias\n",
    "    \n",
    "2. Missing data implicate on model performance\n",
    "    \n",
    "3. not getting generalize model\n",
    "    \n",
    "4. problematic to find value able insight from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fccf2d1-bf5a-40da-be19-94fa79965508",
   "metadata": {},
   "source": [
    "## Q28 . Discuss the pros and cons of imputation methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d6943-276c-4f0a-878c-e9664425c73a",
   "metadata": {},
   "source": [
    "#### Pros & cons of imputation method \n",
    "        \n",
    "1. Numerical feature :if outlier treatment is done we can use mean imputation\n",
    "if outlier treatment has not done we can use median imputation.\n",
    "        \n",
    "2. categorical feature :\n",
    "here we can use mode imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899f8d8-ceb4-4b5c-8330-3d8314a90991",
   "metadata": {},
   "source": [
    "## Q29 How does missing data affect model Performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5b1c9-03e2-4566-a711-e12d58df3679",
   "metadata": {},
   "source": [
    "Due to missing data model will predict incorrect results.or sometime model will not understand data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c43a3-df9c-4679-b266-f4e636b2f713",
   "metadata": {},
   "source": [
    "## Q30. Define imbalanced data in the context of machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b83f0-27dc-4197-a25c-89f7990915ca",
   "metadata": {},
   "source": [
    "When one class has higher percentage of data as compare to another class is called as imbalance data\n",
    "    \n",
    "example :\n",
    "        \n",
    "Majority class ==> 90\n",
    "Minority class ==> 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a952f71-5834-4efc-b852-9d8eef308de8",
   "metadata": {},
   "source": [
    "## Q31 Discuss the challenges posed by imbalanced data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aa4bb-1f58-4299-a559-b3144c4aa95c",
   "metadata": {},
   "source": [
    "Imbalance data can cause problem like biased model, inaccurate prediction. In case of classification\n",
    "when one class is majority class & another class is Minority class so the classification result will be majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff8bb8-3e02-4e68-8fb2-9d7f3be8e97e",
   "metadata": {},
   "source": [
    "## Q32 What techniques can be used to address imbalanced data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721ad5a-b90a-40a9-a2a1-8c9f96d6a423",
   "metadata": {},
   "source": [
    "There are three techniques to address imbalance data :\n",
    "        \n",
    "1.oversampling\n",
    "        \n",
    "2.undersampling\n",
    "        \n",
    "3.SMOTE(synthetic minority oversampling techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54114cf0-5659-4b77-bfd4-4b195e38c328",
   "metadata": {},
   "source": [
    "## Q33 Explain the process of up-sampling and down -sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100be7cc-e86f-4a99-a855-7d8d8cb51afe",
   "metadata": {},
   "source": [
    "Up sampling :\n",
    "\n",
    "In up sampling repeat the data from majority class which is equivalent to minority class\n",
    "        \n",
    "        \n",
    "down Sampling :\n",
    "        \n",
    "In down sampling use the data from majority class which equivalent to miniority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6bb9a-edf9-4918-9584-c78e28e7df1a",
   "metadata": {},
   "source": [
    "## Q34 When would you use up-sampling versus down-sampling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a199ec-8b17-4c6f-a236-19daaebec1d7",
   "metadata": {},
   "source": [
    "If the dataset has an imbalanced distribution, up sampling may be more effective in addressing the problem. Down sampling can result in a loss of important information and may not be effective in balancing the class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b41462-6153-4cdc-8c29-b72c7c6cbc46",
   "metadata": {},
   "source": [
    "## Q35 What is SMOTE and How does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3693a1e-33ce-4288-9b94-4fbadf8c94a5",
   "metadata": {},
   "source": [
    "In SMOTE technique state that same nature of data will be part of same group.SMOTE is an oversampling technique that generates synthetic samples from the minority class. It obtains a synthetically class-balanced or nearly class-balanced training set, then trains the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417927ed-37d1-4e45-9875-c0167220a22d",
   "metadata": {},
   "source": [
    "## Q36 Explain the role of SMOTE in handling imbalanced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f340b68-cb14-400c-9b96-a2d0054deea5",
   "metadata": {},
   "source": [
    "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesis new minority instances between existing minority instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb14fb2-aa4c-4b97-895c-fb3aed5e5780",
   "metadata": {},
   "source": [
    "## Q37. Discuss the advantages and limitations of SMOTE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fada5-ed32-49f9-a8b3-74c1632a2433",
   "metadata": {},
   "source": [
    "A drawback of SMOTE is that it doesn't consider the majority class while creating synthetic samples. Additionally, it can create synthetic samples between samples that represent noise. As a result, the augmented dataset will have more noise than the original one, which can hurt performance.\n",
    "    \n",
    "While SMOTE is highly effective, it's not without its challenges and limitations: Data Quality: SMOTE assumes that the minority class instances are close in feature space. If the minority class is very sparse or if the data quality is poor, the synthetic samples created may not be representative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d66c2-e2c4-4d70-a18d-d426e7f4c605",
   "metadata": {},
   "source": [
    "## Q38 Provide examples of scenarios where SMOTE is beneficial ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3629b-ec21-4d68-b8b3-2374b319250c",
   "metadata": {},
   "source": [
    "SMOTE creates new synthetic spam emails based on existing ones, balancing the dataset for better spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a0484-e01a-4322-86a8-550f2dd3281c",
   "metadata": {},
   "source": [
    "## Q39 Define data interpolation and its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ababa60-f16f-42e8-8b44-3d8cd6efaa3e",
   "metadata": {},
   "source": [
    "interpolation is a process of determining the unknown values that lie in between the known data points. It is mostly used to predict the unknown values for any geographical related data points such as noise level, rainfall, elevation, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee096-3437-4322-ba49-3ead91eac9d6",
   "metadata": {},
   "source": [
    "## Q40 What are the common methods of data interpolations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcaffd5-27d4-41a4-91b6-f26e0def61ef",
   "metadata": {},
   "source": [
    "There are three common method of data interpolation :\n",
    "1. Linear interpolation\n",
    "\n",
    "2. cubic interpolation \n",
    "        \n",
    "3. Polynomial interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d7b96-102c-4b59-8d6a-892019fdc76a",
   "metadata": {},
   "source": [
    "## Q41 Discuss the implications of using data interpolation in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd868c-3adf-43af-95e9-027e23624e33",
   "metadata": {},
   "source": [
    "interpolation refers to the process of estimating unknown values that fall between known data points. This can be useful in various scenarios, such as filling in missing values in a dataset or generating new data points to smooth out a curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd3cdb-eb79-4c39-b4eb-0e3f432623f5",
   "metadata": {},
   "source": [
    "## Q42. What are outliers in a dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b82c8-27a2-43ec-b1ba-04f55202075c",
   "metadata": {},
   "source": [
    "Outlier in dataset is most extreme values are present in dataset is called outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b57366-6004-4fdc-89a5-11cb40af8ebb",
   "metadata": {},
   "source": [
    "## Q43 Explain the impact of outliers in machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1755cc-8f57-47d7-be96-54f2d3dbbc5b",
   "metadata": {},
   "source": [
    "Outlier impact in machine learning model :\n",
    "        \n",
    "    1.Model can be biased due to outliers\n",
    "    \n",
    "    2.model cant give accurate prediction.\n",
    "    \n",
    "    3.reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d7299-c6f6-46e7-bb31-4a0101d7ffb1",
   "metadata": {},
   "source": [
    "## Q44 Discuss techniques for identifying outliers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301cdd3-2937-443f-b261-aa9b77cb01f3",
   "metadata": {},
   "source": [
    "We can find outlier using :\n",
    "    \n",
    "1. Box plot \n",
    "    \n",
    "2. python describe function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6d677-9713-4c02-abe3-e466fc65ff9e",
   "metadata": {},
   "source": [
    "## Q45 How can outliers be handled in a dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d92be2-1086-4a82-ba69-78512d422d13",
   "metadata": {},
   "source": [
    "1. If we want to removed extreme values from data we need to use 5 point summery for outlier treatment we use IQR range find outliers & removed it.\n",
    "\n",
    "2. if we don't want remove outlier from data we can impute it with mean and median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691b655-d1b2-4f8d-9461-6a7cf28ec591",
   "metadata": {},
   "source": [
    "## Q46 Compare and contrast Filter ,Wrapper, and Embedded methods for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865a5dd-6b45-46d7-b32e-5112bba2e53f",
   "metadata": {},
   "source": [
    "#### Filter methods\n",
    "Evaluate features independently of the model, based on their individual characteristics and statistical tests. They're fast and good for large datasets, but don't consider feature relationships. Examples include variance thresholds and information gain.\n",
    "\n",
    "#### Wrapper methods\n",
    "Test different combinations of features to see which performs best for a specific model. They can offer better model performance, but are computationally intensive and time-consuming. Examples include sequential feature selection.\n",
    "\n",
    "#### Embedded methods\n",
    "Select features while training the model itself. They balance efficiency and model-specific learning, and are less prone to overfitting. Examples include LASSO and Ridge.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39eed62-4095-465d-9f85-3a38ff603759",
   "metadata": {},
   "source": [
    "## Q47 Provide examples of algorithms associated with each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4024717-522c-4ca5-bf03-bd32f41be4a1",
   "metadata": {},
   "source": [
    "There are 3 algorithm associated with these method\n",
    "    \n",
    "    1.Ridge regression (l1 regularization)\n",
    "    2.Lasso regression (l2 regularization)\n",
    "    3.Elastic net regression (L1 + l2 regularization)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff00fd-84da-4293-a4a4-a649ed99cc2a",
   "metadata": {},
   "source": [
    "#### Q48 Discuss the advantages and disadvantages of each feature selection method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f01140-3c71-420c-a046-481e487d75e8",
   "metadata": {},
   "source": [
    "#### Filter Method :\n",
    "        \n",
    "- Advantage :\n",
    "\n",
    "    1.computationaly efficient,fast processing ,less prone to overfitting.\n",
    "    \n",
    "- Disadvantage:\n",
    "    \n",
    "    1.less precision could fail to find the feature.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c51d9d-b7f6-45bb-9d27-e6d4e8467c8b",
   "metadata": {},
   "source": [
    "#### Wrapper method :\n",
    "\n",
    "Advantages :\n",
    "\n",
    "1.high precision\n",
    "\n",
    "disadvantage :\n",
    "\n",
    "1.computationally expensive, less precision,tend to over fitting\n",
    "\n",
    "\n",
    "#### Embedded method\n",
    "\n",
    "Advantage :\n",
    "\n",
    "1.Low extra cost \n",
    "\n",
    "disadvantage\n",
    "\n",
    "1.Learning dependant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446a419-b456-4724-8cd2-00f15054e6e6",
   "metadata": {},
   "source": [
    "## Q49 Explain the concept of feature scaling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1c9c1-118c-45ba-9084-f6a37c3b9da8",
   "metadata": {},
   "source": [
    "Feature scaling state that the feature will bring on same scale for modelling purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a0c92-048a-4e5a-b1cb-31dfca2e0fdf",
   "metadata": {},
   "source": [
    "## Q50 Describe the process of standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3d060-47aa-4a91-9e50-eecd1b00ec60",
   "metadata": {},
   "source": [
    "Standardization is a statistical technique used in data preprocessing to make different variables more comparable. It's like translating all these different data “languages” into one universal dialect`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3289d9-6a4d-4564-b6a0-f8625b34f4d8",
   "metadata": {},
   "source": [
    "## Q51 How does mean normalization differ from standardization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bccc0-082a-48e9-93aa-f37b6d4e0077",
   "metadata": {},
   "source": [
    "#### Normalization :\n",
    "        \n",
    "In normalization minimum & maximum value are use for scaling.\n",
    "        \n",
    "#### standardization :\n",
    "        \n",
    "In standardization mean & standard deviation is used for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473b558-85f7-4cd2-8af7-2762bce1a5ff",
   "metadata": {},
   "source": [
    "## Q52 Discuss the advantages and disadvantages of Min-MaX scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092c0ff-7a8a-4cd2-9c98-b2ec47a04852",
   "metadata": {},
   "source": [
    "#### Advantage :   \n",
    "    1.Simple to understand and implement.\n",
    "    2.Effective for a wide range of data.\n",
    "    \n",
    "#### disadvantage :   \n",
    "    1.Sensitive to outlier\n",
    "    2.can lead to information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0ce87-b85e-48fd-875d-c00dea7c0de6",
   "metadata": {},
   "source": [
    "## Q53 What is the Purpose of unit vector scaling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342785f-b322-40ab-bde0-f172ea6caeb0",
   "metadata": {},
   "source": [
    "Unit vector scaling is a technique that can be used to normalize the range of independent variables or features of data. It's also known as feature scaling and is often performed during data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b4ee7-0ca0-431a-ad5a-40a067080f02",
   "metadata": {},
   "source": [
    "## Q54 Define Principle Component Analysis (PCA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671034e-0d0f-457f-876a-c881b3310d83",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc7d79-208f-4bc7-a164-dd33fdfc6069",
   "metadata": {},
   "source": [
    "## Q55 Explain the steps involved in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597291d6-7488-4910-8168-aa8fbed1a03d",
   "metadata": {},
   "source": [
    "1.Standardize the range of continuous initial variables\n",
    "\n",
    "2.Compute the covariance matrix to identify correlations\n",
    "\n",
    "3.Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "\n",
    "4.Create a feature vector to decide which principal components to keep\n",
    "\n",
    "5.Recast the data along the principal components axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d563237-109a-4b78-a9df-daa9667c94bd",
   "metadata": {},
   "source": [
    "## Q56 Discuss the significance of eigenvalues and eigenvectors in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade44626-bc81-41ed-8d93-03493ad31ffc",
   "metadata": {},
   "source": [
    "To determine the most significant principal components, you can rank the eigenvectors from highest to lowest eigenvalue. The first eigenvector is the main principal component, and subsequent eigenvectors are orthogonal to it so that they can span the entire x-y area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4fad1-d570-4496-aa1a-532640aa05b3",
   "metadata": {},
   "source": [
    "## Q57 How does PCA helps in dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2ad30-b6fc-4746-9e05-5673c0aae199",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a linear dimensionality reduction technique that reduces the number of dimensions in a data set while retaining most of its information. It does this by transforming the original variables into a smaller set of new, uncorrelated variables called principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fc145-1042-430e-be37-56530518ebe6",
   "metadata": {},
   "source": [
    "## Q58 Define data encoding and its importance in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478aec83-07d4-4cb4-837d-43b1d5557320",
   "metadata": {},
   "source": [
    "Data encoding is the process of converting data, usually categorical or text data, into a numerical format that machines can understand and process. It's a crucial step in preparing data for machine learning algorithms, as these algorithms primarily work with numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681ff8b-0cea-40ac-a527-baa68fca7052",
   "metadata": {},
   "source": [
    "## Q59 Explain Nominal Encoding and provide an example ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4858048-4552-41aa-aadf-a6c061fc638d",
   "metadata": {},
   "source": [
    "In nominal encoding convert categorical data into numerical. no order or rank in the data.\n",
    "    \n",
    "Example : marital status ,gender\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb0dc0-bc66-4e21-8b1c-1d3def1729b8",
   "metadata": {},
   "source": [
    "## Q60 Discuss the process of One Hot Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70853631-0306-4ed7-8b6f-bcef9ac4d5a0",
   "metadata": {},
   "source": [
    "- In onehot Encoding Categorical data convert into numerical data. \n",
    "- There is no order in data.\n",
    "    \n",
    "Example : Single,Married, In relationship\n",
    "    \n",
    "    \n",
    "| Single | Married | In Relationship |\n",
    "|--------|---------|-----------------|\n",
    "|   1    |    0    |        0        |\n",
    "|   0    |    1    |        0        |\n",
    "|   0    |    0    |        1        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867beeeb-50b6-4c07-bb67-748d3800120b",
   "metadata": {},
   "source": [
    "## Q61 How do you handle multiple categories in One Hot Encoding!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ebe77-2592-4991-8b52-285b43949079",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique that converts categorical variables into numerical variables by creating a new binary variable for each possible value of the categorical variable. When dealing with multiple categories, you can:\n",
    "Limit encoding to frequent labels\n",
    "Encode only the 10 most frequent labels, and group all other labels under a new category.\n",
    "Drop a dummy variable\n",
    "Since dummy variables contain redundant information, you can drop one of the newly created columns to avoid the dummy variable trap.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8430d-d88a-4463-8346-17d507c9b2aa",
   "metadata": {},
   "source": [
    "## Q62 Explain Mean Encoding and its advantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0c820-8b0f-46a8-9192-db26d522e27c",
   "metadata": {},
   "source": [
    "Target encoding, also known as mean encoding, involves replacing each category with the mean (or some other statistic) of the target variable for that category. Here's how target encoding works: Calculate the mean of the target variable for each category. Replace the category with its corresponding mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade4c65-3cf0-47d8-93cb-1044689620ee",
   "metadata": {},
   "source": [
    "## Q63 Provide examples of Ordinal Encoding and Label Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e5f47-de01-4044-9c40-bbeada13a5f9",
   "metadata": {},
   "source": [
    "Assign numerical label to each category\n",
    "    \n",
    "Example : colors :- red ,green,blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383bc594-f93b-45f2-a126-1cfd29eece14",
   "metadata": {},
   "source": [
    "## Q64 What is Target Guided Ordinal Encoding and how is it used ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fc67a-8c96-4839-ac1e-e66fbc1c73db",
   "metadata": {},
   "source": [
    "Target-guided ordinal encoding is a technique used to encode categorical variables for machine learning models. This encoding technique is particularly useful when the target variable is ordinal, meaning that it has a natural order, such as low, medium, and high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30eafc-f080-4423-b988-32f4c5331d0c",
   "metadata": {},
   "source": [
    "## Q65 Define covariance and its significance in statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f448f0-ae0d-4098-a014-c43140f856b8",
   "metadata": {},
   "source": [
    "covariance is a statistical tool that measures the relationship between two random variables and how much they change together. It can indicate the direction of a relationship, such as whether the variables tend to move in tandem or show an inverse relationship, but it does not indicate the strength of the relationship or the dependency between the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0c755-0c0d-45d0-b9d6-6a4c85fd1648",
   "metadata": {},
   "source": [
    "## Q66 Explain the process of correlation check ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c23b6-e5b9-43e6-a0ca-607cde25d636",
   "metadata": {},
   "source": [
    "Correlation can be check between two random variable it should be positive or negative\n",
    "by using heat map or corr() method in python we can check linear relationship between two feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85f8bb-3c10-457c-be42-a0fab3bbd785",
   "metadata": {},
   "source": [
    "## Q67 What is the Pearson Correlation Coefficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc06cc6-79ef-4a3c-851a-fd8ab8dd9662",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d5699-41c3-4370-80f9-9c321dfa7fc0",
   "metadata": {},
   "source": [
    "## Q68 How does Spearmans Rank Correlation differ from Pearson's Correlation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f7156-221f-4c19-9594-17a68532375c",
   "metadata": {},
   "source": [
    "- Spearman's rank correlation:\n",
    "\n",
    "A nonparametric measure that assesses how well a monotonic function describes the relationship between two variables. It's often used for ordinal data, non-normally distributed continuous data, or data with outliers. Spearman's correlation orders values from highest to lowest, without considering the distances between them. A perfect Spearman correlation of +1 or −1 occurs when each variable is a perfect monotone function of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3793164-77c1-48a6-bd29-ef8f6c9f5afa",
   "metadata": {},
   "source": [
    "- Pearson's correlation\n",
    "\n",
    "\n",
    "Assesses linear relationships between quantitative variables that follow a normal distribution. It's typically used for jointly normally distributed data. Pearson's correlation compares the mean value of the product of the standard scores of matched pairs of observations. Positive values indicate a positive correlation, negative values indicate a negative correlation, and zero values indicate no correlation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225ad59-0b46-4ad1-aca4-0ab281021a5f",
   "metadata": {},
   "source": [
    "## Q69 Discuss the importance of Variance Inflation Factor (VIF) in feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe4746-52e4-4a26-ba9b-97ed243c1d54",
   "metadata": {},
   "source": [
    "The variance inflation factor (VIF) is a statistical tool that measures how much the variance of an estimated regression coefficient is increased due to collinearity. It's a useful tool for feature selection in machine learning analysis because it can help identify and eliminate variables that cause multicollinearity. Multicollinearity can lead to unstable parameter estimation, weak predictive ability, and less dependable statistical conclusions. Removing multicollinearity can improve the accuracy and sensitivity of classification models, and the stability and generalization performance of extreme learning machines (ELM) models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd29b4-c7de-446a-a35d-b70e6fa519b7",
   "metadata": {},
   "source": [
    "## Q70 Define feature selection and its purpose ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363f146-3435-4cc3-8402-1b6e4095229a",
   "metadata": {},
   "source": [
    " The goal of feature selection is to find the best set of features from the available data that models the given problem to yield a machine learning model with good performance and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ed7eb-39ed-4305-82e9-e2dbd9031b4c",
   "metadata": {},
   "source": [
    "## Q71 Explain the process of Recursive Feature Elimination ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de762a-3122-4d04-8858-6d349baf34e0",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination is a feature selection method to identify a dataset's key features. The process involves developing a model with the remaining features after repeatedly removing the least significant parts until the desired number of features is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4852b-dc32-47bd-b0c8-fa3c8fb941fc",
   "metadata": {},
   "source": [
    "## Q72 How does Backward Elimination work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2456e-0a7c-404e-9606-3762c379dfb4",
   "metadata": {},
   "source": [
    "Backward elimination is a more systematic approach that starts with a complete set of features and removes features one by one until the model performance reaches a peak. This method is more computationally efficient but may not find the optimal set of features either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46b3eb-1de5-44b2-97ee-25f9c1a0b02d",
   "metadata": {},
   "source": [
    "## Q73 Discuss the advantages and limitations of Forward Elimination ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029df8d8-8635-44e6-a713-fe521cc16404",
   "metadata": {},
   "source": [
    "- Advantages :\n",
    "\n",
    "1.The most fundamental solution algorithm.\n",
    "\n",
    "2.Basis for computing inverse; can solve multiple sets of equations.\n",
    "    \n",
    "- limitation:\n",
    "\n",
    "1.Solution of one set of linear equations at a time.\n",
    "    \n",
    "2.Less efficient for a single set of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88686a3-25a6-45f5-a3db-cc336927c24f",
   "metadata": {},
   "source": [
    "## Q74 What is feature engineering and why is it important ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e61d1-4d91-4968-bc54-e09acac44d81",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in machine learning that involves selecting and transforming raw data into features that can better represent an underlying problem for a predictive model. The goal is to improve model accuracy by providing more relevant and meaningful information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7501634-8905-4961-9eff-813c69541979",
   "metadata": {},
   "source": [
    "## Q75 Discuss the steps involved in feature engineering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac0f9b-2787-455f-83e8-ddeb58a5165c",
   "metadata": {},
   "source": [
    "Step in feature engineering\n",
    "    \n",
    "    1. deal with unique value\n",
    "    2. handling Nan values\n",
    "    3. handling outliers\n",
    "    4. encoding\n",
    "    5. aggregate feature \n",
    "    6. feature selection\n",
    "    7. EDA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927193b2-f0e4-45c9-be0f-c90d05e304a8",
   "metadata": {},
   "source": [
    "## Q76 Provide example of feature engineering techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901d2e8-efa7-4ca7-8d6a-b7ae467a433c",
   "metadata": {},
   "source": [
    "1.Feature extraction\n",
    "    \n",
    "2.Imputation \n",
    "    \n",
    "3.scaling\n",
    "    \n",
    "4.handling missing values\n",
    "    \n",
    "5.handling outliers\n",
    "    \n",
    "6.onehot encoding\n",
    "    \n",
    "7.log transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3089f-4049-460a-ab23-4362f1d87c10",
   "metadata": {},
   "source": [
    "## Q77 How does feature selection differ from feature engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e957c8-0a46-4fd1-8982-4b83483210e2",
   "metadata": {},
   "source": [
    "Feature engineering and feature selection are both important techniques used in machine learning to improve model accuracy and performance. They have different objectives, but they can overlap and are often used together in a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327e468-a1c2-4978-a53a-c2f931f82388",
   "metadata": {},
   "source": [
    "## Q78 Explain the importance of feature selection in machine learning pipelines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67045723-6527-42ff-a311-44de76ce2e22",
   "metadata": {},
   "source": [
    "In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b38367-4f1b-4ac4-a26c-af55c02ddcdc",
   "metadata": {},
   "source": [
    "## Q79 Discuss the impact of feature selection on model performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23968743-2e59-4614-8656-75d492fa5d1a",
   "metadata": {},
   "source": [
    "In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb458e7a-f6fc-461b-8b38-96107cd3c39d",
   "metadata": {},
   "source": [
    "## Q80 How do you determine which features to include in a machine-learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996cce5-7eea-4153-bd21-84dc9ed8ce08",
   "metadata": {},
   "source": [
    "For feature selection process we have used 2 method VIF Method and RFE method to select best \n",
    "feature for model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
