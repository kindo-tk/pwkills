{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q20-What do you mean by Measure of Central Tendency and Measures of Dispersion .How it can be calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. measure of centeral tendancy\n",
    "2. measure of dispersion\n",
    "\n",
    "In the measure of centeral tendancy :=\n",
    "\n",
    "1. mean \n",
    "2. median \n",
    "3. mode\n",
    "\n",
    "\n",
    "In the measure of dispersion :-\n",
    "\n",
    "1. range\n",
    "2. percentile\n",
    "3. quartile\n",
    "4. IQR\n",
    "5. std\n",
    "6. variance\n",
    "\n",
    "`Mean` ==> Avarage value of data.\n",
    "mean = sum(xi)/N\n",
    "\n",
    "\n",
    "`median` ==> mid point value of data.\n",
    "Ex - [1,2,3,4,5]  ==> 3 is median\n",
    "     [1,2,3,4,5,6] ===> (3+4)/2 is median\n",
    "\n",
    "\n",
    "`Mode` ==> most frequent value of data.\n",
    "\n",
    "ex . [1,2,3,1,2,3,1] ==> mode is 1\n",
    "\n",
    "\n",
    "`variance` ==> variance mean the spread of data how data point are vary from mean.\n",
    " var = sum(xi -x`)/(n-1)\n",
    "\n",
    "\n",
    "`std deviation` ==> the measure of distribution of statistical data.\n",
    " sigma = root(variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. What do you mean by skewness.Explain its types.Use graph to show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    " Skewness Right and left skew:==>\n",
    "\n",
    "![](Skewness-of-a-distribution-Large.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given -middle digrams\n",
    "\n",
    "if skew ness =0 so mean = median = mode that is called as normal or gausssion distrubution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given Left side digram :\n",
    "\n",
    "if tail is available on right side of data that type of data is called as right skewed data. in these data mean >=median>=mode so its is right skewed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given right side digram==>\n",
    "\n",
    "if tail is available on left side of data that type of data is called as left skewed data. in these data mean<=median<=mode so its is left skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Explain PROBABILITY MASS FUNCTION (PMF) and PROBABILITY DENSITY FUNCTION (PDF). and what is the difference between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilty Density function :\n",
    "In probability theory, a probability density function (PDF), density function, or density of an absolutely continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample.Probability density is the probability per unit length, in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability Mass function :\n",
    "In probability and statistics, a probability mass function (sometimes called probability function or frequency function) is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between PDF & PMF\n",
    "\n",
    "\n",
    "1. PDF is used continuous random variable.\n",
    "2. PMF is used discrete variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23.What is correlation. Explain its type in details.what are the methods of determining correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "correlation means finding the relationship between toi random variable. If the slope of the line is negative, the two variables follow a negative correlation. If the slope is positive, it is a positive correlation. \n",
    "\n",
    "##### types of correlations :-\n",
    "\n",
    "1.pearson correlation\n",
    "\n",
    "2.spearman correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pearson correlation:--\n",
    "The first person to give a mathematical formula for the measurement of the degree of relationship between two variables in 1890 was Karl Pearson. Karl Pearson’s Coefficient of Correlation is also known as Product Moment Correlation or Simple Correlation Coefficient. This method of measuring the coefficient of correlation is the most popular and is widely used. It is denoted by ‘r’, where r is a pure number which means that r has no unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spearman Correlation:--\n",
    "\n",
    "Spearman’s Rank Correlation Coefficient or Spearman’s Rank Difference Method or Formula is a method of calculating the correlation coefficient of qualitative variables and was developed in 1904 by Charles Edward Spearman. In other words, the formula determines the correlation coefficient of variables like beauty, ability, honesty, etc., whose quantitative measurement is not possible. Therefore, these attributes are ranked or put in the order of their preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. Calculate coefficient of correlation between the marks obtained by 10 students in Accountancy and statistics:\n",
    "\n",
    "| Student      | 1  | 2  | 3  | 4  | 5  | 6  | 7   | 8  | 9  | 10 |\n",
    "|--------------|----|----|----|----|----|----|-----|----|----|----|\n",
    "| Accountancy  | 45 | 70 | 65 | 30 | %  | 40 | 350 | 75 | 85 | 60 |\n",
    "| Statistics   | 35 | 90 | 70 | 40 | 9  | 40 | 60  | 80 | 80 | 50 |\n",
    "\n",
    "\n",
    "Use Karl Pearson's Coefficient of Correlation Method to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Student=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "Accountancy=[45, 70, 65 ,30,90, 40, 350, 75, 85, 60,]\n",
    "Statistics=[35 ,90, 70, 40, 90, 40, 60, 80, 80, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df =pd.DataFrame(data=list(zip(Student,Accountancy,Statistics)),columns=[\"Student\",\"Accountancy\",\"Statistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Accountancy</th>\n",
       "      <th>Statistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student  Accountancy  Statistics\n",
       "0        1           45          35\n",
       "1        2           70          90\n",
       "2        3           65          70\n",
       "3        4           30          40\n",
       "4        5           90          90"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Accountancy</th>\n",
       "      <th>Statistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Student</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.236692</td>\n",
       "      <td>0.107431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accountancy</th>\n",
       "      <td>0.236692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistics</th>\n",
       "      <td>0.107431</td>\n",
       "      <td>0.130903</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Student  Accountancy  Statistics\n",
       "Student      1.000000     0.236692    0.107431\n",
       "Accountancy  0.236692     1.000000    0.130903\n",
       "Statistics   0.107431     0.130903    1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr() ## Pearson corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. student & accountancy 23%  Positively correlated.\n",
    "2. accountancy & statistics is 13% postive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Discuss the 4 differences between correlation and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Feature**                                           | **Correlation**                                                                                     | **Regression**                                                                                      |\n",
    "|-------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| **Definition**                                        | Determines the interconnection or co-relationship between the variables.                            | Explains how an independent variable is numerically associated with the dependent variable.         |\n",
    "| **Variables**                                         | No distinction between independent and dependent variables; both are treated equally.               | Distinguishes between dependent and independent variables, treating them differently.               |\n",
    "| **Objective**                                         | To find a quantitative value expressing the association between the variables.                      | To calculate the values of a dependent variable based on the values of an independent variable.     |\n",
    "| **Movement of Variables**                             | Indicates the degree to which both variables move together.                                          | Specifies the effect of a change in the independent variable on the dependent variable.             |\n",
    "| **Application**                                       | Helps to understand the strength and direction of the connection between two variables.             | Helps in predicting or estimating the value of a dependent variable based on the value of an independent variable. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Find the most likely price at Delhi corresponding to the price of Rs. 70 at Agra from the following data: Coefficient of correlation between the prices of the two places +0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :\n",
    "\n",
    "Data:\n",
    "\n",
    "Correlation Coefficient (Agra & Delhi) = +0.8\n",
    "\n",
    "Price at Agra = Rs. 70 (given)\n",
    "\n",
    "Let's assume a standard deviation (SD) of 1 for both Agra and Delhi (this can be adjusted if actual values are available).\n",
    "\n",
    "Calculate Covariance:\n",
    "\n",
    "Covariance = Correlation Coefficient * SD(Agra) * SD(Delhi)\n",
    "\n",
    "Covariance = 0.8 * 1 * 1 = 0.8\n",
    "\n",
    "Estimate Mean Price at Delhi:\n",
    "\n",
    "We can estimate the mean price at Delhi based on the price at Agra and the covariance.\n",
    "\n",
    "Mean(Delhi) = Price(Agra) + (Covariance / SD(Agra))\n",
    "\n",
    "Mean(Delhi) = Rs. 70 + (0.8 / 1) = Rs. 70.8\n",
    "\n",
    "Therefore, based on the given correlation and assumed standard deviations, the estimated price for the same item in Delhi is Rs. 70.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. In a partially destroyed laboratory record of an analysis of correlation data, the following results only are legible: Variance of x = 9, Regression equations are: (i) 8x-10y = -66; (ii) 40x - 18y = 214. What are (a) the mean values of x and y, (b) the coefficient of correlation between x and y, (c) the o of y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Regression Equations:\n",
    "\n",
    "The given equations represent two possible regression lines:\n",
    "\n",
    "(i) 8x - 10y = -66 (likely regression of y on x)\n",
    "(ii) 40x - 18y = 214 (likely regression of x on y)\n",
    "\n",
    "Coefficient of Correlation (r):\n",
    "Even though we have two regression equations, both contain the same variables (x and y). We can leverage this to find the correlation coefficient (r). Here's how:\n",
    "\n",
    "The slope of the regression line (b) relates to the correlation coefficient through the formula:\n",
    "\n",
    "b_YX (slope of y on x) = - r * (SD_x / SD_y)\n",
    "\n",
    "b_XY (slope of x on y) = r * (SD_y / SD_x)\n",
    "\n",
    "Since both slopes are negative inverses of each other (given by the - sign), it implies a positive correlation between x and y.\n",
    "\n",
    "We can equate the expressions for b_YX and b_XY (as both represent the correlation coefficient):\n",
    "(-10 / 8) = r * (1 / SD_y) = r * (SD_x / 18)\n",
    "\n",
    "Unfortunately, due to missing standard deviations, we cannot solve for the exact value of r. However, we know it's positive based on the negative inverses of the slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. What is Normal Distribution? What are the four Assumptions of Normal Distribution? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a normal distribution, mean (average), median (midpoint), and mode (most frequent observation) are equal. These values represent the peak or highest point. The distribution then falls symmetrically around the mean, the width of which is defined by the standard deviation.\n",
    "\n",
    "#### The assumptions made for a normal distribution include:\n",
    "\n",
    "`Unimodal and Symmetric`: The normal distribution is symmetric around its mean, with the mean, median, and mode all being equal. The distribution is bell-shaped and unimodal, meaning it has only one peak.\n",
    "\n",
    "`Continuous Random Variable`: The normal distribution is defined for continuous random variables. It is not appropriate for discrete data.\n",
    "\n",
    "`Mean and Variance`: The distribution is fully described by its mean (μ) and variance (σ^2). The mean specifies the center of the distribution, and the variance determines the spread or dispersion around the mean.\n",
    "\n",
    "`Independence`: If the data is a sample from a population, the observations should be independent of each other. This means that one observation should not influence another.\n",
    "Fixed Shape: The shape of the normal distribution is fixed, regardless of the mean and variance values. This means that once the mean and variance are specified, the shape of the distribution is determined.\n",
    "\n",
    "`No Outliers`: The normal distribution assumes that there are no outliers in the data that significantly skew the results. Outliers can have a large impact on the estimation of the mean and variance.\n",
    "\n",
    "`No Skewness or Kurtosis`: The normal distribution assumes that the data is not skewed (asymmetrical) or kurtotic (heavy-tailed) but instead follows a symmetric bell-shaped curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29.Write all the characteristics or Properties of the Normal Distribution Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distributions are symmetric, unimodal, and asymptotic, and the mean, median, and mode are all equal. A normal distribution is perfectly symmetrical around its center. That is, the right side of the center is a mirror image of the left side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30.Which of the following options are correct about Normal Distribution Curve.\n",
    "\n",
    "(a) Within a range 0.6745 of o on both sides the middle 50% of the observations occur j,e. mean £0.67450\n",
    "covers 50% area 25% on each side.\n",
    "\n",
    "(b) Mean #15.D, (i,e.) + lo) covers 68.268% area, 34.134 % area lies on either side of the mean.\n",
    "\n",
    "(c) Mean +#25.D. (ie. + 20) covers 25.45% area, 47.725% area lies on either side of the mean.\n",
    "\n",
    "(d) Mean +3 S.D. (ie. p +30) covers 99.73% area, 49.856% area lies on the either side of the mean.\n",
    "\n",
    "(e) Only 0.27% area is outside the range u +30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :\n",
    "Correct points are\n",
    "\n",
    "1)b\n",
    "\n",
    "2)c\n",
    "\n",
    "3)d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. The mean of a distribution is 60 with a standard deviation of 10. Assuming that the distribution is normal, what percentage of items be (i) between 60 and 72, (ii) between 50 and 60, (iii) beyond 72 and (iv) between 70 and 80?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a normal distribution, we use the **Z-score** formula to find the percentage of items within certain ranges. The Z-score tells us how many standard deviations a particular value is from the mean.\n",
    "\n",
    "### Given:\n",
    "- Mean (μ) = 60\n",
    "- Standard Deviation (σ) = 10\n",
    "\n",
    "### Z-score Formula:\n",
    "\\[\n",
    "Z = (X - μ) / σ\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(X\\) is the value\n",
    "- \\(μ\\) is the mean\n",
    "- \\(σ\\) is the standard deviation\n",
    "\n",
    "### (i) Percentage of items between 60 and 72:\n",
    "\n",
    "To find the Z-scores:\n",
    "- For \\(X = 60\\):  \n",
    "  Z = (60 - 60) / 10 = 0\n",
    "\n",
    "- For \\(X = 72\\):  \n",
    "  Z = (72 - 60) / 10 = 1.2\n",
    "\n",
    "Using a standard normal distribution table or calculator:\n",
    "- The percentage of data below \\(Z = 0\\) is 50%.\n",
    "- The percentage of data below \\(Z = 1.2\\) is approximately 88.49%.\n",
    "\n",
    "So, the percentage of items between 60 and 72 is:\n",
    "88.49% - 50% = 38.49%\n",
    "\n",
    "### (ii) Percentage of items between 50 and 60:\n",
    "\n",
    "To find the Z-scores:\n",
    "- For \\(X = 50\\):  \n",
    "  Z = (50 - 60) / 10 = -1\n",
    "\n",
    "- For \\(X = 60\\):  \n",
    "  Z = (60 - 60) / 10 = 0\n",
    "\n",
    "Using a standard normal distribution table:\n",
    "- The percentage of data below \\(Z = -1\\) is approximately 15.87%.\n",
    "- The percentage of data below \\(Z = 0\\) is 50%.\n",
    "\n",
    "So, the percentage of items between 50 and 60 is:\n",
    "50% - 15.87% = 34.13%\n",
    "\n",
    "### (iii) Percentage of items beyond 72:\n",
    "\n",
    "To find the Z-score:\n",
    "- For \\(X = 72\\):  \n",
    "  Z = (72 - 60) / 10 = 1.2\n",
    "\n",
    "Using a standard normal distribution table:\n",
    "- The percentage of data below \\(Z = 1.2\\) is approximately 88.49%.\n",
    "\n",
    "So, the percentage of items beyond 72 is:\n",
    "100% - 88.49% = 11.51%\n",
    "\n",
    "### (iv) Percentage of items between 70 and 80:\n",
    "\n",
    "To find the Z-scores:\n",
    "- For \\(X = 70\\):  \n",
    "  Z = (70 - 60) / 10 = 1\n",
    "\n",
    "- For \\(X = 80\\):  \n",
    "  Z = (80 - 60) / 10 = 2\n",
    "\n",
    "Using a standard normal distribution table:\n",
    "- The percentage of data below \\(Z = 1\\) is approximately 84.13%.\n",
    "- The percentage of data below \\(Z = 2\\) is approximately 97.72%.\n",
    "\n",
    "So, the percentage of items between 70 and 80 is:\n",
    "97.72% - 84.13% = 13.59%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. What is the statistical hypothesis? Explain the errors in hypothesis testing.b)Explain the Sample. What are Large Samples & Small Samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical hypothesis: A statement about the nature of a population. It is often stated in terms of a population parameter. Null hypothesis: A statistical hypothesis that is to be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in hypothesis testing :\n",
    "\n",
    "Two potential types of statistical error are Type I error (α, or level of significance), when one falsely rejects a null hypothesis that is true, and Type II error (β), when one fails to reject a null hypothesis that is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samples :\n",
    "In statistical context, a sample is considered to be large if it is at least 30. On the other hand, a sample is considered small is it is less than than 30. This rule is based on the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large and Small sample theory. Large sample theory. The sample size n is greater than 30 (n≥30) it is known as large sample. For large samples the sampling distributions of statistic are normal(Z test). A study of sampling distribution of statistic for large sample is known as large sample theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 37.100 students of a PW IOI obtained the following grades in Data Science paper:\n",
    "Grade :[A, B, C, D, E]\n",
    "\n",
    "Total Frequency :[15, 17, 30, 22, 16, 100]\n",
    "\n",
    "Using the y 2 test, examine the hypothesis that the distribution of grades is uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 7.7\n",
      "p-value: 0.10320672172612926\n",
      "Degrees of freedom: 4\n",
      "Fail to reject the null hypothesis: The distribution of grades is uniform.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Observed frequencies\n",
    "observed_frequencies = np.array([15, 17, 30, 22, 16])\n",
    "\n",
    "# Expected frequencies (uniform distribution)\n",
    "expected_frequencies = np.array([20, 20, 20, 20, 20])\n",
    "\n",
    "# Calculate the chi-square test statistic and p-value\n",
    "chi2_statistic, p_value = chisquare(observed_frequencies, f_exp=expected_frequencies)\n",
    "\n",
    "# Degrees of freedom\n",
    "df = len(observed_frequencies) - 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Chi-square statistic: {chi2_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Degrees of freedom: {df}\")\n",
    "\n",
    "# Determine if we reject the null hypothesis\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The distribution of grades is not uniform.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The distribution of grades is uniform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 38.Anova Test:\n",
    "\n",
    "To study the performance of three detergents and three different water temperatures the following\n",
    "\n",
    "whiteness readings were obtained with specially designed equipment.\n",
    "\n",
    "| Water Temp   | Detergents A | Detergents B | Detergents C |\n",
    "|--------------|--------------|--------------|--------------|\n",
    "| Cold Water   | 57           | 55           | 67           |\n",
    "| Warm Water   | 49           | 52           | 68           |\n",
    "| Hot Water    | 54           | 46           | 58           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "data = {\n",
    "    'Water Temp': ['Cold Water', 'Worm Water', 'Hot Water'],\n",
    "    'Detergent A': [57, 55, 67],\n",
    "    'Detergent B': [49, 52, 68],  \n",
    "    'Detergent C': [54, 46, 58]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Water Temp</th>\n",
       "      <th>Detergent A</th>\n",
       "      <th>Detergent B</th>\n",
       "      <th>Detergent C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cold Water</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worm Water</td>\n",
       "      <td>55</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hot Water</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Water Temp  Detergent A  Detergent B  Detergent C\n",
       "0  Cold Water           57           49           54\n",
       "1  Worm Water           55           52           46\n",
       "2   Hot Water           67           68           58"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 0.6029143897996357\n",
      "p-value: 0.5773005027709032\n",
      "Fail to reject null hypothesis: There is no evidence of significant difference.\n"
     ]
    }
   ],
   "source": [
    "fvalue, pvalue = stats.f_oneway(df['Detergent A'], df['Detergent B'], df['Detergent C'])\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", fvalue)\n",
    "print(\"p-value:\", pvalue)\n",
    "\n",
    "# Interpretation (adjust based on significance level):\n",
    "if pvalue < 0.05:\n",
    "    print(\"Reject null hypothesis: There is a significant difference between detergent means.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: There is no evidence of significant difference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 39.How would you create a basic Flask route that displays \"Hello, World!\" on the homepage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from flask import Flask\n",
    "\n",
    "@app.route(\"/homepage\")\n",
    "\n",
    "def Main():\n",
    "\n",
    "  return \"Hello World !\"\n",
    "    \n",
    "    \n",
    "from above example we can create flask route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40.Explain how to set up a Flask application to handle form submissions using POST requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form data is submitted using a POST method to the '/submission' route, which is routed to 'submission. html' through the flask app's trigger function and the forwarded form data. Here' request', which is imported from Flask, is used to access the form data in a flask application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41.Write a Flask route that accepts a parameter in the URL and displays it on the page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`12.0.0.1:5000/?name =Hello world`  by using these route we can pass paramter in URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42.How can you implement user authentication in a Flask application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the Flask-Login library for session management\n",
    "2. Use the built-in Flask utility for hashing passwords\n",
    "3. Add protected pages to the app for logged in users only\n",
    "4. Use Flask-SQLAlchemy to create a User model\n",
    "5. Create sign-up and login forms for the users to create accounts and log in\n",
    "6. Flash error messages back to users when something goes wrong\n",
    "7. Use information from the user’s account to display on the profile page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43.Describe the process of connecting a Flask app to a SQLite database using SQLAIchemy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQlite is installed on Linux systems by default, and is installed as part of the Python package on Windows.\n",
    "    \n",
    "Prerequisites.\n",
    "\n",
    "Step 1 — Installing Flask and Flask-SQLAlchemy.\n",
    "\n",
    "Step 2 — Setting up the Database and Model.\n",
    "\n",
    "Step 3 — Displaying All Records.\n",
    "\n",
    "Step 4 — Displaying a Single Record.\n",
    "\n",
    "Step 5 — Creating a New Record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44.How would you create a RESTful API endpoint in Flask that returns JSON data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from flask import Flask, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "data = {\n",
    "    \"message\": \"Hello, world!\",\n",
    "    \"items\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "@app.route('/api/data', methods=['GET'])\n",
    "def get_data():\n",
    "    return jsonify(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "We can create restful api using json data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45.Explain how to use Flask-WTF to create and validate forms in a Flask application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask-WTF is a Flask extension that makes working with web forms in Flask applications easier. It is based on the WTForms library, which provides a powerful and flexible way to define and validate forms.\n",
    "Here's a step-by-step guide on using Flask-WT\n",
    "\n",
    "1. Installation:\n",
    "Install Flask-WTF using pip: pip install Flask-WTF\n",
    "\n",
    "2. Creating a Form:\n",
    "Create a Python class that inherits from FlaskForm. Inside this class, define the form fields using WTForms field types.\n",
    "\n",
    "\n",
    "3. Using the Form in a Template:\n",
    "In your Flask route, create an instance of the form and pass it to the template\n",
    "\n",
    "4. Validating Form Data:\n",
    "The validate_on_submit() method in your route handles form validation. If the form is submitted and all validations pass, it returns True. Otherwise, it returns False, and you can access the validation errors in the template using form.field_name.errors.\n",
    "Flask-WTF simplifies the process of creating and validating forms in Flask applications significantly. It provides a clean and easy-to-use interface for working with forms, making your web applications more robust and secure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46.How can you implement file uploads in a Flask application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import debug \n",
    "\n",
    "from fileinput import filename \n",
    "\n",
    "from flask import *\n",
    "\n",
    "app = Flask(__name__) \n",
    "\n",
    "@app.route('/') \n",
    "\n",
    "def main(): \n",
    "\treturn render_template(\"index.html\") \n",
    "\n",
    "@app.route('/success', methods = ['POST']) \n",
    "\n",
    "def success(): \n",
    "\tif request.method == 'POST': \n",
    "\t\tf = request.files['file'] \n",
    "\t\tf.save(f.filename) \n",
    "\t\treturn render_template(\"Acknowledgement.html\", name = f.filename) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "\tapp.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Series & Datatrames ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature          | Series                                | DataFrame                          |\n",
    "|------------------|---------------------------------------|------------------------------------|\n",
    "| **Definition**   | A one-dimensional array-like object   | A two-dimensional table-like structure |\n",
    "| **Data Structure** | Single column                         | Multiple columns                    |\n",
    "| **Indexing**     | Has a single index                    | Has both row and column indices     |\n",
    "| **Data Type**    | Holds a single data type per Series   | Can hold different data types in different columns |\n",
    "| **Creation**     | Created from a list, array, or dictionary | Created from lists, dictionaries, or other DataFrames |\n",
    "| **Usage**        | Represents a single column of data    | Represents a table of data with multiple columns |\n",
    "| **Access**       | Access elements by index or label     | Access elements by row and column indices |\n",
    "| **Operations**   | Operations are applied element-wise   | Operations can be applied to rows or columns as a whole |\n",
    "| **Example**      | `s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])` | `df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})` |\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "- **Series**:\n",
    "  - Represents a single column or a one-dimensional array.\n",
    "  - Indexed by a single axis (index).\n",
    "  - Used when you only need a single column of data.\n",
    "\n",
    "- **DataFrame**:\n",
    "  - Represents a table of data with multiple columns.\n",
    "  - Indexed by two axes (row and column indices).\n",
    "  - Used for handling and analyzing data with multiple attributes or features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between loc and iloc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature           | `loc`                                          | `iloc`                                        |\n",
    "|-------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Indexing**      | Label-based indexing                          | Integer-based indexing                        |\n",
    "| **Access**        | Access data using labels (row names and column names) | Access data using integer positions (row and column indices) |\n",
    "| **Syntax**        | `df.loc[row_label, column_label]`             | `df.iloc[row_index, column_index]`            |\n",
    "| **Row Access**    | Access rows using index labels                | Access rows using integer positions           |\n",
    "| **Column Access** | Access columns using column labels            | Access columns using integer positions        |\n",
    "| **Range**         | Supports label-based ranges                   | Supports integer-based ranges                 |\n",
    "| **Error Handling**| Throws an error if the label is not found      | Throws an error if the index is out of range  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [5, 6, 7, 8],\n",
    "    'C': [9, 10, 11, 12]\n",
    "}\n",
    "index = ['a', 'b', 'c', 'd']\n",
    "df = pd.DataFrame(data, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B   C\n",
       "a  1  5   9\n",
       "b  2  6  10\n",
       "c  3  7  11\n",
       "d  4  8  12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    1\n",
       "B    5\n",
       "C    9\n",
       "Name: a, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a']        # Selects row with index 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a', 'B']   # Selects value in row 'a' and column 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B   C\n",
       "a  1  5   9\n",
       "b  2  6  10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a':'b']    # Selects rows from 'a' to 'b' (inclusive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    2\n",
       "c    3\n",
       "d    4\n",
       "Name: A, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'A']     # Selects all rows for column 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    5\n",
      "C    9\n",
      "Name: a, dtype: int64\n",
      "5\n",
      "   A  B   C\n",
      "a  1  5   9\n",
      "b  2  6  10\n",
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "d    4\n",
      "Name: A, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])        # Selects the first row (index 0)\n",
    "print(df.iloc[0, 1])     # Selects value in the first row and second column (index 0, 1)\n",
    "print(df.iloc[0:2])      # Selects rows from index 0 to 1 (exclusive of 2)\n",
    "print(df.iloc[:, 0])     # Selects all rows for the first column (index 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Supervised and Unsupervised Learning\n",
    "+ Supervised Learning:\n",
    "\n",
    "The algorithm is trained on labeled data.\n",
    "The goal is to learn a mapping from inputs to outputs.\n",
    "Examples include regression and classification.\n",
    "\n",
    "\n",
    "+ Unsupervised Learning:\n",
    "\n",
    "The algorithm is trained on unlabeled data.\n",
    "The goal is to find hidden patterns or intrinsic structures.\n",
    "Examples include clustering and dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Bias-Variance Tradeoff\n",
    "\n",
    "+ Bias:\n",
    "\n",
    "Error due to overly simplistic assumptions in the learning algorithm.\n",
    "High bias can cause underfitting.\n",
    "\n",
    "+ Variance:\n",
    "\n",
    "Error due to too much complexity in the learning algorithm.\n",
    "High variance can cause overfitting.\n",
    "\n",
    "+ Tradeoff:\n",
    "\n",
    "A model with low bias and low variance is ideal.\n",
    "In practice, increasing model complexity decreases bias but increases variance and vice versa.\n",
    "The goal is to find the optimal balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Precision and Recall\n",
    "\n",
    "+ Precision:\n",
    "\n",
    "The ratio of true positive predictions to the total predicted positives.\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "+ Recall:\n",
    "\n",
    "The ratio of true positive predictions to the actual positives.\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "+ Accuracy:\n",
    "\n",
    "The ratio of correct predictions to the total predictions.\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is Overfitting and How to Prevent It?\n",
    "\n",
    "+ Overfitting:\n",
    "\n",
    "When a model learns the noise in the training data instead of the actual pattern.\n",
    "It performs well on the training data but poorly on new data.\n",
    "\n",
    "+ Prevention:\n",
    "\n",
    "Use cross-validation.\n",
    "Simplify the model (reduce complexity).\n",
    "Use regularization techniques.\n",
    "Collect more training data.\n",
    "Prune the model (in decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is  Cross-Validation Concept:\n",
    "\n",
    "A technique to evaluate the generalizability of a model.\n",
    "The dataset is divided into k subsets (folds).\n",
    "The model is trained on k-1 folds and tested on the remaining fold.\n",
    "This process is repeated k times, each time with a different fold as the test set.\n",
    "The results are averaged to get a more accurate evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Classification and Regression\n",
    "\n",
    "+ Classification:\n",
    "\n",
    "Predicts discrete class labels.\n",
    "Examples: Spam detection, disease diagnosis.\n",
    "\n",
    "+ Regression:\n",
    "\n",
    "Predicts continuous values.\n",
    "Examples: House price prediction, temperature forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the concept of Ensemble Learning\n",
    "\n",
    "+ Concept:\n",
    "Combines multiple models to improve performance.\n",
    "Types include bagging, boosting, and stacking.\n",
    "Reduces overfitting and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the difference between batch gradient descent and stochastic gradient descent.\n",
    "\n",
    "+ Gradient Descent\n",
    "Concept:\n",
    "An optimization algorithm to minimize the cost function.\n",
    "Iteratively adjusts the parameters in the direction of the negative gradient.\n",
    "Types: Batch Gradient Descent, Stochastic Gradient Descent, Mini-batch Gradient Descent.\n",
    "\n",
    "+ Batch Gradient Descent vs. Stochastic Gradient Descent\n",
    "Batch Gradient Descent:\n",
    "\n",
    "Uses the entire dataset to compute the gradient.\n",
    "Converges to the minimum but can be slow and requires a lot of memory.\n",
    "Stochastic Gradient Descent:\n",
    "\n",
    "Uses a single training example to compute the gradient.\n",
    "Faster and more memory-efficient.\n",
    "Can be noisy and may not converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the curse of dimensionality in machine learning?\n",
    "Concept:\n",
    "As the number of features increases, the volume of the feature space increases exponentially.\n",
    "Data points become sparse, and the distance between them increases.\n",
    "Models become harder to train and may suffer from overfitting.\n",
    "+ L1 vs. L2 Regularization\n",
    "+ L1 Regularization (Lasso):\n",
    "\n",
    "Adds the absolute values of the coefficients to the cost function.\n",
    "Can result in sparse models (feature selection).\n",
    "+ L2 Regularization (Ridge):\n",
    "\n",
    "Adds the squared values of the coefficients to the cost function.\n",
    "Tends to shrink coefficients but doesn’t set them to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a confusion matrix and how is it used? Define AUC-ROC curve\n",
    "\n",
    "+ Concept:\n",
    "A table used to evaluate the performance of a classification model.\n",
    "Contains true positives, true negatives, false positives, and false negatives.\n",
    "Helps calculate precision, recall, accuracy, and other metrics.\n",
    "\n",
    "+ AUC-ROC Curve\n",
    "Concept:\n",
    "ROC (Receiver Operating Characteristic) curve plots true positive rate vs. false positive rate.\n",
    "AUC (Area Under the Curve) measures the area under the ROC curve.\n",
    "AUC-ROC indicates the model's ability to distinguish between classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the k-nearest neighbors algorithm.\n",
    "\n",
    "Concept:\n",
    "A non-parametric, instance-based learning algorithm.\n",
    "Classifies a data point based on the majority class of its k nearest neighbors.\n",
    "Uses distance metrics like Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the basic concept of a Support Vector Machine (SVM).How does the kernel trick work in SVM?\n",
    "\n",
    "Concept:\n",
    "A supervised learning algorithm for classification and regression.\n",
    "Finds the optimal hyperplane that maximizes the margin between classes.\n",
    "Uses support vectors (data points closest to the hyperplane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the different types of kernels used in SVM and when would you use each? \n",
    "\n",
    "Transforms data into a higher-dimensional space to make it linearly separable.\n",
    "- Common kernels: Linear, Polynomial, Radial Basis Function (RBF).\n",
    "Types of Kernels in SVM\n",
    "- Linear Kernel:\n",
    "Used when the data is linearly separable.\n",
    "- Polynomial Kernel:\n",
    "Suitable for non-linear data.\n",
    "- RBF Kernel:\n",
    "Handles complex, non-linear data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the hyperplane in SVM and how is it determined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperplane in SVM\n",
    "Concept:\n",
    "The decision boundary separating different classes.\n",
    "Determined by maximizing the margin between support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the pros and cons of using a Support Vector Machine (SVM)?\n",
    "Pros:\n",
    "\n",
    "Effective in high-dimensional spaces.\n",
    "Works well with a clear margin of separation.\n",
    "Cons:\n",
    "\n",
    "Not suitable for large datasets.\n",
    "Sensitive to the choice of kernel and parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the difference between a hard margin and a soft margin SVM,\n",
    "Hard Margin:\n",
    "\n",
    "No misclassification allowed.\n",
    "Works only if data is linearly separable.\n",
    "Soft Margin:\n",
    "\n",
    "Allows some misclassification to handle non-linear data.\n",
    "Controlled by a parameter (C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the process of constructing a decision tree\n",
    "\n",
    "Starts with the entire dataset as the root.\n",
    "Splits the data based on the feature that provides the highest information gain.\n",
    "Repeats the process recursively for each child node until a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the working principle of a decision tree,\n",
    "\n",
    "Concept:\n",
    "Splits data into subsets based on feature values.\n",
    "Each node represents a feature, and each branch represents a decision.\n",
    "Leaf nodes represent the final class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is information gain and how is it used in decision trees?\n",
    "+ Information Gain\n",
    "Concept:\n",
    "Measures the reduction in entropy after splitting the data.\n",
    "Used to decide the best feature for splitting.\n",
    "Information Gain = Entropy(parent) - Weighted Sum of Entropy(children)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Gini impurity and its role in decision trees.\n",
    "+ Gini Impurity\n",
    "Concept:\n",
    "Measures the likelihood of incorrect classification of a randomly chosen element.\n",
    "Used to create splits in decision trees.\n",
    "Gini Impurity = 1 - Sum of (probability of each class)^2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the advantages and disadvantages of decision trees?\n",
    "+ Advantages and Disadvantages of Decision Trees\n",
    "Advantages:\n",
    "\n",
    "Easy to interpret and visualize.\n",
    "Can handle both numerical and categorical data.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to overfitting.\n",
    "Sensitive to small changes in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do random forests improve upon decision trees?\n",
    "Concept:\n",
    "An ensemble method that constructs multiple decision trees.\n",
    "Aggregates their predictions to improve accuracy and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a random forest algorithm work?\n",
    "\n",
    "\n",
    "Creates multiple bootstrap samples from the training data.\n",
    "Builds a decision tree for each sample.\n",
    "Aggregates the predictions (majority voting for classification, averaging for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is bootstrapping in the context of random forests?\n",
    "\n",
    "Random sampling with replacement.\n",
    "Each tree is trained on a different subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the concept of feature importance in random forests.\n",
    "\n",
    "Measures the importance of each feature in making predictions.\n",
    "Calculated based on the reduction in impurity (Gini or entropy).\n",
    "\n",
    "Logistic Regression Model\n",
    "Concept:\n",
    "A linear model used for binary classification.\n",
    "Uses the logistic function (sigmoid) to map predictions to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the key hyperparameters of a random forest and how do they affect the model?t\n",
    "\n",
    "\n",
    "Number of trees (n_estimators): More trees increase accuracy but also computational cost.\n",
    "Maximum depth (max_depth): Limits the depth of the trees to prevent overfitting.\n",
    "Minimum samples split (min_samples_split): Minimum number of samples required to split an internal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the logistic regression model and its assumptions.\n",
    "Concept:\n",
    "Uses the logistic function to model the probability of the default class.\n",
    "Thresholding the probability to make a final classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does logistic regression handle binary classification problems?\n",
    "\n",
    "Maps input values to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the sigmoid function and how is it used in logistic regression?\n",
    "\n",
    "- The sigmoid function transforms a linear combination of features into a probability.\n",
    "- It is crucial in logistic regression for converting the output of the linear model into a probability score between 0 and 1.\n",
    "- The function's output is interpreted as the probability of belonging to the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the concept of the cost function in logistic regression.\n",
    "\n",
    "\n",
    "Measures the difference between predicted and actual values.\n",
    "Logistic regression uses the binary cross-entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can logistic regression be extended to handle multiclass classification?\n",
    "\n",
    "Uses one-vs-rest or softmax (multinomial logistic regression) to handle multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between LI and L2 regularization in logistic regression?\n",
    "\n",
    "\n",
    "L1 Regularization (Lasso): Adds the absolute values of coefficients.\n",
    "\n",
    "L2 Regularization (Ridge): Adds the squared values of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is XGBoost and how does it differ frorm other boosting algorithms?\n",
    "\n",
    "An optimized gradient boosting algorithm.\n",
    "Faster and more efficient than traditional boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the concept of boosting in the context of ensemble learning.\n",
    "+ How does XGBoost handle missing values?\n",
    "\n",
    "Sequentially builds models that correct errors of previous models.\n",
    "Reduces bias and variance.\n",
    "+ Handling Missing Values in XGBoost\n",
    "\n",
    "Automatically handles missing data by learning the best imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the key hyperparameters in XGBoost and how do they affect model performance?\n",
    "\n",
    "\n",
    "Learning rate (eta): Controls the contribution of each tree.\n",
    "\n",
    "Number of trees (n_estimators): Controls the number of boosting rounds.\n",
    "\n",
    "Maximum depth (max_depth): Limits the depth of individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the process of gradient boosting in XGBoost.\n",
    "\n",
    "Builds models sequentially, each one correcting the errors of its predecessor.\n",
    "Uses gradient descent to minimize the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the advantages and disadvantages of using XGBoost?\n",
    " \n",
    "+ Advantages:\n",
    "\n",
    "High performance and speed.\n",
    "Handles missing data well.\n",
    "Supports regularization.\n",
    "\n",
    "+ Disadvantages:\n",
    "\n",
    "Complex to tune.\n",
    "Can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
